<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Eye Tracking with FaceMesh</title>
    <style lang="css">
      body {
        font-family: sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        background-color: #f0f0f0;
        text-align: center;
      }

      #intro {
        padding: 1rem;
        background-color: tomato;
        border-radius: 5px;
        border: 1px solid;
      }

      button {
        cursor: pointer;
      }

      #video,
      #canvas {
        display: none;
        position: fixed;
        inset: 0;
        width: 100%;
        height: 100%;
        object-fit: cover;
      }

      #video {
        z-index: -1;
      }

      #canvas {
        z-index: 1;
      }
    </style>
  </head>
  <body>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs" defer></script>
    <script
      src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"
      defer
    ></script>
    <div id="intro">
      <p>To use this website, please allow camera access.</p>
      <button onclick="startWebcam()">Access Camera</button>
    </div>
    <video
      id="video"
      autoplay
      playsinline
      muted
      width="1280"
      height="720"
    ></video>
    <canvas id="canvas" width="1280" height="720"></canvas>

    <script defer>
      const video = document.getElementById("video");
      const canvas = document.getElementById("canvas");
      const intro = document.getElementById("intro");
      const ctx = canvas.getContext("2d");
      let model;

      // Indizes fÃ¼r rechtes und linkes Auge
      const RIGHT_EYE = [33, 133];
      const LEFT_EYE = [362, 263];

      async function startWebcam() {
        try {
          model = await facemesh.load();

          const stream = await navigator.mediaDevices.getUserMedia({
            video: {
              width: { ideal: 1280, max: 1920 },
              height: { ideal: 720, max: 1080 },
            },
          });
          intro.style.display = "none";
          video.srcObject = stream;
          video.onloadeddata = () => detectEyes();
          video.style.display = "block";
          canvas.style.display = "block";
        } catch (error) {
          console.error("Error:", error);
        }
      }

      async function detectEyes() {
        if (!model || !video) return;

        const predictions = await model.estimateFaces(video);

        ctx.clearRect(0, 0, canvas.width, canvas.height);

        predictions.forEach((prediction) => {
          const keypoints = prediction.scaledMesh;

          // Rechtes Auge zeichnen
          RIGHT_EYE.forEach((index) => {
            const [x, y] = keypoints[index];
            drawPoint(x, y);
          });

          // Linkes Auge zeichnen
          LEFT_EYE.forEach((index) => {
            const [x, y] = keypoints[index];
            drawPoint(x, y);
          });
        });

        requestAnimationFrame(detectEyes);
      }

      function drawPoint(x, y) {
        ctx.beginPath();
        ctx.arc(x, y, 20, 0, 20 * Math.PI);
        ctx.fillStyle = "yellow";
        ctx.fill();
      }
    </script>
  </body>
</html>
